{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import networkit as ntkit  # pip install networkit\n",
    "import datetime\n",
    "import igraph   # primero: sudo apt install build-essential python-dev libxml2 libxml2-dev zlib1g-dev\n",
    "# luego: pip install pyhton-igraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levanto toda la data\n",
    "files = os.listdir(\"/home/digitas_arg/fede_ego\")\n",
    "files  = [x for x in files if('.xlsx' in x)]\n",
    "files = [x for x in files if('fedevigevani.xlsx' not in x)]\n",
    "\n",
    "data = pd.DataFrame([],columns = ['author', 'date', 'mentions', 'permalink', 'text'])\n",
    "\n",
    "for i,f in enumerate(files):\n",
    "    aux = pd.read_excel('/home/digitas_arg/fede_ego/'+f)\n",
    "    data = data.append(aux,ignore_index = True)\n",
    "    if((i%500) == 0):\n",
    "        print(i)\n",
    "\n",
    "# Apendeo la data del influencer original\n",
    "data = data.append(pd.read_excel(\"./data/fedevigevani.xlsx\"),ignore_index = False)\n",
    "data = data.append(pd.read_excel(\"./data/fedevigevani.2.xlsx\"),ignore_index = False)\n",
    "\n",
    "# Saco donde no hay menciones\n",
    "data = data.dropna(subset = [\"mentions\"] )\n",
    "data = data.loc[data.mentions != ' ']\n",
    "data = data.loc[data.mentions != '']\n",
    "\n",
    "# Save por las moscas\n",
    "data.to_excel(\"./data/fede_total.xlsx\",index = False)  # Guardo por las MOSCAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levanto y genero las relaciones\n",
    "data = pd.read_excel(\"./data/fede_total.xlsx\") \n",
    "\n",
    "#data = data.append(pd.read_excel(\"./data/fedevigevani.xlsx\"),ignore_index = False)\n",
    "#data = data.append(pd.read_excel(\"./data/fedevigevani.2.xlsx\"),ignore_index = False)\n",
    "\n",
    "#data = data.dropna(subset = [\"mentions\"] )\n",
    "#data = data.loc[data.mentions != ' ']\n",
    "#data = data.loc[data.mentions != '']\n",
    "\n",
    "# =============================================================================\n",
    "#  Me armo las relaciones y me genero un dataframe sumarizado con comentatios y menciones\n",
    "# =============================================================================\n",
    "\n",
    "def generate_relations(data):\n",
    "    data[\"mentions\"] = data.mentions.apply(lambda x: re.findall(r'\\S+',x))\n",
    "    \n",
    "    data_ampliado = data.mentions.apply(pd.Series).stack().rename('mentions').reset_index()\n",
    "    print(\"data_ampliado generado\")\n",
    "    data_rankme = pd.merge(data_ampliado,data,left_on='level_0',right_index=True, suffixes=(['','_old']))[data.columns]\n",
    "    print(\"data_rankme generado\")\n",
    "    \n",
    "    rank = data_rankme[['author',\"mentions\"]].groupby(['author',\"mentions\"]).size().reset_index()\n",
    "    rank.columns = [\"author\",\"mentions\",\"weight\"]\n",
    "    return rank\n",
    "\n",
    "relations = generate_relations(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorto las conexiones pesadas para que sea mayor a 3. Luego me quedo con el subgrafo\n",
    "net = nx.from_pandas_edgelist(relations,'author','mentions',[\"weight\"])\n",
    "\n",
    "def trim_edges(g, weight=1):\n",
    "    g2=nx.Graph()\n",
    "    for f, to, edata in g.edges(data=True):\n",
    "        if edata['weight'] > weight:\n",
    "            g2.add_edge(f,to,weight = edata['weight'])\n",
    "    return g2\n",
    "\n",
    "net_trim3 = trim_edges(net,3)\n",
    "\n",
    "l_graphs = list(nx.connected_component_subgraphs(net_trim3))\n",
    "l_graphs.sort(key=len)\n",
    "print(\"length de los subgrafo 1\",l_graphs[-1])\n",
    "print(\"length de los subgrafo 2\",l_graphs[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardo la data por las dudas\n",
    "net_trim3 = l_graphs[-1]\n",
    "nx.write_pajek(net_trim3,'./outputs/fede.net')\n",
    "nx.write_gml(net_trim3,'./outputs/fede.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# levanto la data para laburar con las librerias de networkx y igraph\n",
    "net_trim3 = nx.read_pajek('./outputs/fede.net')\n",
    "g = igraph.Graph.Read_GML(\"./outputs/fede.gml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralidad: \n",
    "### Uso networkit ya que esta codeado en C++ y anda mucho mas rapido. De todas formas dejo el betweeness paralelizado :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# betweeness paralelizado! Igual uso el de networkit\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Divide a list of nodes `l` in `n` chunks\"\"\"\n",
    "    l_c = iter(l)\n",
    "    while 1:\n",
    "        x = tuple(itertools.islice(l_c, n))\n",
    "        if not x:\n",
    "            return\n",
    "        yield x\n",
    "\n",
    "\n",
    "def _betmap(G_normalized_weight_sources_tuple):\n",
    "    \"\"\"Pool for multiprocess only accepts functions with one argument.\n",
    "    This function uses a tuple as its only argument. We use a named tuple for\n",
    "    python 3 compatibility, and then unpack it when we send it to\n",
    "    `betweenness_centrality_source`\n",
    "    \"\"\"\n",
    "    return nx.betweenness_centrality_source(*G_normalized_weight_sources_tuple)\n",
    "\n",
    "\n",
    "def betweenness_centrality_parallel(G, processes=None):\n",
    "    \"\"\"Parallel betweenness centrality  function\"\"\"\n",
    "    p = Pool(processes=processes)\n",
    "    node_divisor = len(p._pool) * 4\n",
    "    node_chunks = list(chunks(G.nodes(), int(G.order() / node_divisor)))\n",
    "    num_chunks = len(node_chunks)\n",
    "    bt_sc = p.map(_betmap,\n",
    "                  zip([G] * num_chunks,\n",
    "                      [True] * num_chunks,\n",
    "                      [None] * num_chunks,\n",
    "                      node_chunks))\n",
    "\n",
    "    # Reduce the partial solutions\n",
    "    bt_c = bt_sc[0]\n",
    "    for bt in bt_sc[1:]:\n",
    "        for n in bt:\n",
    "            bt_c[n] += bt[n]\n",
    "    return bt_c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = ntkit.nxadapter.nx2nk(net_trim3)  # adapto networkit a networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ntkit.centrality.DegreeCentrality(G)\n",
    "d.run()\n",
    "print(\"Degree\")\n",
    "c= ntkit.centrality.ApproxCloseness(G,nSamples=20000)\n",
    "c.run()\n",
    "b= ntkit.centrality.Betweenness(G)\n",
    "b.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me armo una tabla con los usuarios que tuvieron la mayor de las 3 metricas\n",
    "def sorted_map(map):\n",
    "    ms = sorted(map, key=map.__getitem__, reverse=True)\n",
    "    return ms\n",
    "\n",
    "dict_degree = {}\n",
    "[dict_degree.update( {str(name) : d } ) for name,d in zip(net_trim3.nodes(),d.scores())]\n",
    "dict_betweeness= {}\n",
    "[dict_betweeness.update( {str(name) : b } ) for name,b in zip(net_trim3.nodes(),b.scores())]\n",
    "dict_closeness = {}\n",
    "[dict_closeness.update( {str(name) : c } ) for name,c in zip(net_trim3.nodes(),c.scores())]\n",
    "\n",
    "ds=sorted_map(dict_degree)\n",
    "cs=sorted_map(dict_closeness)\n",
    "bs=sorted_map(dict_betweeness)\n",
    "\n",
    "names1= ds[:30]\n",
    "names2= bs[:30]\n",
    "names3= cs[:30]\n",
    "\n",
    "## use Python sets to compute a union of the sets\n",
    "names=list(set(names1) | set(names2) | set (names3))\n",
    "## build a table with centralities\n",
    "table=pd.DataFrame([{'node_name':name,'degree':dict_degree[name],'centrality':dict_closeness[name],\n",
    "        'betweeness':dict_betweeness[name]} for name in names])\n",
    "\n",
    "table.to_excel(\"centrality.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizo las comunidades usando infomap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo las comunidades\n",
    "comunities = g.community_infomap(edge_weights = 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []\n",
    "\n",
    "for i in range(len(comunities)):\n",
    "    if(len(comunities[i]) > 700):\n",
    "        clusters.append(comunities[i])\n",
    "        \n",
    "name_converter = {}\n",
    "_ = [name_converter.update({i:str(name)}) for i,name in enumerate(net_trim3.nodes())]\n",
    "\n",
    "clusters = sorted(clusters,key = len,reverse = True)\n",
    "[len(x) for x in clusters][0:10]  # Reviso cuales son los length necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influencer_name = 'fedevigevani'\n",
    "\n",
    "# Funcon que entrega las relaciones entre clsuters n_1 y n_2\n",
    "def generate_weight(clusters,n_1,n_2): \n",
    "    l_links = []\n",
    "    community_name = [name_converter.get(x,x) for x in clusters[n_1]]\n",
    "    for name_1 in [name_converter.get(x,x) for x in clusters[n_2]]:\n",
    "            l_links.append(sum([ net_trim3[name_1][x][0]['weight'] for x in net_trim3[name_1] if (x in community_name)]))\n",
    "\n",
    "    return sum(l_links)\n",
    "\n",
    "# Itero entre todas las posibles combinaciones y me genero un grafo de clusters\n",
    "cluster_graph = nx.Graph()\n",
    "for n_1 in range(len(clusters)):\n",
    "    for n_2 in range(len(clusters)):\n",
    "        if(n_1 != n_2):\n",
    "            weight = generate_weight(clusters,n_1,n_2)\n",
    "            if(weight > 0):\n",
    "                cluster_graph.add_edge(n_2,n_1)\n",
    "                cluster_graph[n_2][n_1]['weight'] = weight\n",
    "                        \n",
    "# Agrego las conexiones con el influencer\n",
    "for n in range(len(clusters)):\n",
    "    community_name = [name_converter.get(x,x) for x in clusters[n]]\n",
    "    weight = sum([net_trim3[influencer_name][x][0]['weight']  for x in net_trim3[influencer_name] if (x in community_name)]) \n",
    "    if(weight > 0):\n",
    "        cluster_graph.add_edge(influencer_name,n)\n",
    "        cluster_graph[influencer_name][n]['weight'] = weight\n",
    "\n",
    "# Agrego el size de cada clsuter, con el agregado del influencer\n",
    "aux = {}\n",
    "[aux.update({i:len(n_cluster) for i,n_cluster in enumerate(clusters)})]\n",
    "aux.update({influencer_name:int(np.mean([len(x) for x in clusters]))})   # le asigno la media para que cuando haga el gradiente de colores quede bien.\n",
    "nx.set_node_attributes(cluster_graph,aux,'size')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Muestro la data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nx.draw(cluster_graph)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Me guardo las comunidades. Esto es opcional\n",
    "name_converter = {}\n",
    "[name_converter.update({i:str(name)}) for i,name in enumerate(net_trim3.nodes())]\n",
    "\n",
    "for n in range(len(clusters)):\n",
    "    comunidad1 = pd.DataFrame([],columns = [\"node_number\",\"node_screen\",\"degree\"])\n",
    "    comunidad1[\"node_number\"] = clusters[n]\n",
    "    comunidad1[\"node_screen\"] = comunidad1[\"node_number\"].apply(lambda x: name_converter.get(x,x))\n",
    "    comunidad1[\"degree\"] = comunidad1[\"node_screen\"].apply(lambda x: dict_degree.get(x,x))\n",
    "    comunidad1 = comunidad1.sort_values(by = [\"degree\"],ascending=False)\n",
    "    comunidad1.to_csv(\"./clusters_fede/cluster_\"+str(n)+'.csv')\n",
    "#comunidad1.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreto que tiene cada cluster\n",
    "name_converter = {}\n",
    "[name_converter.update({i:str(name)}) for i,name in enumerate(net_trim3.nodes())]\n",
    "dict_degree = nx.degree_centrality(net_trim3)\n",
    "\n",
    "list_df_clusters = []\n",
    "\n",
    "for n in range(len(clusters)):\n",
    "    comunidad1 = pd.DataFrame([],columns = [\"node_number\",\"node_screen\",\"degree\"])\n",
    "    comunidad1[\"node_number\"] = clusters[n]\n",
    "    comunidad1[\"node_screen\"] = comunidad1[\"node_number\"].apply(lambda x: name_converter.get(x,x))\n",
    "    comunidad1[\"degree\"] = comunidad1[\"node_screen\"].apply(lambda x: dict_degree.get(x,x))\n",
    "    comunidad1 = comunidad1.sort_values(by = [\"degree\"],ascending=False)\n",
    "    list_df_clusters.append(comunidad1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para visualizar:\n",
    "n = 5\n",
    "print(len(clusters[n]))\n",
    "list_df_clusters[n].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de cluster por separado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le agrego nombres a los clusters\n",
    "\n",
    "mapping = {0:'Busca Fama',1:'Youtubers',2:'Amor',3:'Gammers',4:'Youtuber',5:'Youtubers2',6:'Girl Power',7:'Busca Fama',\n",
    "          influencer_name:influencer_name}\n",
    "cluster_graph = nx.relabel.relabel_nodes(cluster_graph,mapping)\n",
    "\n",
    "# Genero el grafo\n",
    "nx.write_gexf(cluster_graph, \"./outputs/ClustersLena.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para ir viendo quienes son los nexos de malena a los clusters\n",
    "n = 14\n",
    "community_name = [name_converter.get(x,x) for x in clusters[n]]\n",
    "[print(x, net_trim3['LenaNarvay'][x][0]['weight'] ) for x in net_trim3['LenaNarvay'] if (x in community_name)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densidad y diametro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntkit.graph.Graph.density(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.effective_size(net_trim3)   # demasiado lento...\n",
    "p = ntkit.distance.EffectiveDiameterApproximation(G)\n",
    "p.run()\n",
    "print(\"Effective diameter: \",p.getEffectiveDiameter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ntkit.distance.Diameter(G)\n",
    "p.run()\n",
    "p.getDiameter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nx.diameter(net_trim3)  # revisar esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar esto:\n",
    "net.Graph(net.ego_graph(net_trim3,'LenaNarvay', radius=2))\n",
    "#nx.average_clustering(net_trim3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
